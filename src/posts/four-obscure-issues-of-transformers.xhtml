<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<meta charset="UTF-8" />
<head>
	<title>Ariel's Domain</title>
	<link rel="stylesheet" href="styles.css" />
</head>
<body>
	<div class="topbar">
		<a href="../index.xhtml">home</a>
		<a href="../blog.xhtml">blog</a>
		<a href="../stories.xhtml">stories</a>
		<a href="../links.xhtml">links</a>
		<a href="../contact.xhtml">contact</a>
	</div>
	<div class="contents">
NAME: Four Obscure Issues of Transformers<br />
DATE: 2024-07-27<br />
TAGS: AI, Transformers, chat bots, LLMs<br />
<br />
1. Introduction<br />
<br />
Heed my words, mortal, for today I shall regale thee with yet another tale of woe and frustration. `But Ariel, why is it not in the ``stories'' category?' I hear thee think. The reason is simple: this tale is not a narrative of human folly,  but rather a chronicle of the shortcomings of those most vaunted Large Language Models, built upon the Transformers' framework.<br />
<br />
As those of sound mind are well aware, LLMs are highly imperfect. The most egregious issues, well-known to all, are hallucinations, difficulties in verifying sources, inconsistency and incoherence... Instead, I wish to tell thee of  the more obscure problems that plague the Transformers architecture.<br />
<br />
2. What is Transformers?<br />
<br />
To begin, allow me to offer some minimal insight into the Transformers architecture. Traditional models of language comprehension, such as Recurrent Neural Networks, process words in a linear sequence, one by one. Alas, this  approach hath its limitations, for it struggleth to capture the subtle relationships between words that span great distances. The Transformers architecture, an innovation of recent time, attempteth to rectify this issue by  processing the entirety of a sentence at once, considering all words simultaneously. Furthermore, Transformers models represent text as tokens, small units of speech, such as words, punctuation marks, or morphemes. These  peculiarities shall prove relevant anon.<br />
<br />
`Why should I, an ordinary reader, care about the Transformers architecture?&#x27; See, foolish mortal, Transformers is the very foundation of modern LLMs. Almost every chat bot thou mayest encounter is a Transformers model - including  ChatGPT, Gemini, Claude, and most other random vector generators. Thus, if the Transformers architecture is beset by an issue, most LLMs shall inherit it.<br />
<br />
3. Obscure issues<br />
<br />
3.1. Learning<br />
<br />
Transformers models are incapable of continuous learning. Any feedback provided by a user is but a fleeting moment, lost to the void once the conversation hath ended. The only permanent source of knowledge for a model is its  training dataset, which serveth as a severe limitation its capacity for growth and improvement. For this reason, among many others, Transformers models can never achieve that much desired `general intelligence&#x27; which delusional fools unsparingly prophecise.<br />
<br />
3.2. Cryptography<br />
<br />
Ah, LLMs and cryptography... A match... Never made, I daresay. Even the monoalphabetic substitution cypher proveth too great a challenge for these much revered models:<br />
GPT-4o-mini<br />
<img src="./cryptography-gpt-4o-mini.png" /><br />
Llama 3.1 70B<br />
<img src="./cryptography-llama-3.1-70b.png" /><br />
Perplexity<br />
<img src="./cryptography-perplexity.png" /><br />
The original message was `Is this to be my last entry in the journal?&#x27; For comparison,
<a href="https://quipqiup.com/">quipqiup.com</a> can decrypt the cyphertext with ease:<br />
<img src="./transformers-vs-quipqiup.png" /><br />
<br />
The most enjoyable aspect of this deficiency is that it is impossible to resolve - Transformers models, having forsaken sequential processing, are forever prevented from learning the operations necessary for basic cryptography. Tokenisation is another issue, for it operateth on the assumption that the input is coherent text or, for instance, code. Expectedly, it faileth immediately upon encountering these bizarre strings of data known as `cyphertexts&#x27;.<br />
<br />
3.3. Creativity<br />
<br />
`Can a robot write a symphony? Can a robot take a blank canvas and turn it into a masterpiece?&#x27; No, it cannot. As glorified statistics engines, LLMs are incapable of generating original ideas. Regardless of the context, they shall produce but the average of what hath worked in similar cases, never daring to venture beyond the boundaries of  the mundane.<br />
<br />
I intend to write a future post regarding the poor writing abilities of LLMs. For now, I do encourage thee to test this on thine own and inform me of the results
(<a href="../contact">contact</a>). It would be interesting to make a comparison.<br />
<br />
3.4. Constructed languages<br />
<br />
Contrary to their name, Large Language Models are, in truth, woefully inept when it comes to navigating the complexities of language.<br />
<br />
Do witness these feeble attempts at constructed language translation
(<a href="./transformers-conlang-prompt.txt">prompt</a>):<br />
GPT-4o-mini<br />
<img src="./transformers-conlang.png" /><br />
The following is the correct answer: `pjungu rinsundaba i shishing'. This is an almost trivial example, for translating languages with fusional morphology is even more difficult for these models.<br />
<br />
Once again, direct thy thanks at non-sequential process and tokenisation, along with the lovely generalisation (in)abilities of LLMs.<br />
<br />
4. Conclusion<br />
<br />
Stop using Transformers.<br />
<br /><br />
<div class="go-back"><a href="../blog.xhtml">&lt;&lt;&lt; back to posts</a></div>
	</div>
</body>
</html>
